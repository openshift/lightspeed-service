llm_providers:
  - name: my_bam
    type: bam
    url: "https://bam-api.res.ibm.com"
    credentials_path: bam_api_key.txt
    models:
      - name: ibm/granite-3-8b-instruct
        context_window_size: 8000
        parameters:
          max_tokens_for_response: 500
    tlsSecurityProfile:
      type: Custom
      ciphers:
          - TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
          - TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
      minTLSVersion: VersionTLS13
  - name: my_openai
    type: openai
    url: "https://api.openai.com/v1"
    credentials_path: openai_api_key.txt
    models:
      - name: gpt-4-1106-preview
      - name: gpt-4o-mini
  - name: my_azure_openai
    type: azure_openai
    url: "https://myendpoint.openai.azure.com/"
    credentials_path: azure_openai_api_key.txt
    api_version: "2024-02-15-preview"
    deployment_name: my_azure_openai_deployment_name
    models:
      - name: gpt-4o-mini
  - name: my_watsonx
    type: watsonx
    url: "https://us-south.ml.cloud.ibm.com"
    credentials_path: watsonx_api_key.txt
    project_id: XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX
    models:
      - name: ibm/granite-3-8b-instruct
  - name: my_rhoai
    type: rhoai_vllm
    url: "http://localhost:8000/v1"
    credentials_path: rhoai_api_key.txt
    models:
      - name: mistral-7b-instruct-v0.3
  - name: my_rhelai
    type: rhelai_vllm
    url: "http://localhost:8000/v1"
    credentials_path: rhelai_api_key.txt
    models:
      - name: granite-3-8b-instruct
  - name: instructlab
    type: openai
    url: "http://localhost:8000/v1"
    credentials_path: openai_api_key.txt
    models:
      - name: merlinite-7b-lab-Q4_K_M
ols_config:
  # max_workers: 1
  reference_content:
#    product_docs_index_path: "./vector_db/ocp_product_docs/4.15"
#    product_docs_index_id: ocp-product-docs-4_15
#    embeddings_model_path: "./embeddings_model"
  conversation_cache:
    type: memory
    memory:
      max_entries: 1000
  logging_config:
    app_log_level: info
    lib_log_level: warning
    uvicorn_log_level: info
    suppress_metrics_in_log: false
    suppress_auth_checks_warning_in_log: false
  default_provider: my_bam
  default_model: ibm/granite-3-8b-instruct
  expire_llm_is_ready_persistent_state: -1
  # query_filters:
  #   - name: foo_filter
  #     pattern: '\b(?:foo)\b'
  #     replace_with: "deployment"
  #   - name: bar_filter
  #     pattern: '\b(?:bar)\b'
  #     replace_with: "openshift"
  # query_validation_method chooses, how the first query will be validated
  # supported values:
  #     "keyword"  - keyword based query validation (see ols/utils/keywords.py)
  #     "llm"      - LLM based query validation
  #     "disabled" - question validation is disabled (all questions will be marked as valid)
  query_validation_method: llm
  authentication_config:
    module: "k8s"
    k8s_cluster_api: "https://api.example.com:6443"
    k8s_ca_cert_path: "/Users/home/ca.crt"
    skip_tls_verification: false
  user_data_collection:
    feedback_disabled: false
    feedback_storage: "/tmp/data/feedback"
    transcripts_disabled: false
    transcripts_storage: "/tmp/data/transcripts"
  tls_config:
    tls_certificate_path: /app-root/certs/certificate.crt
    tls_key_path: /app-root/certs/private.key
    tls_key_password_path: /app-root/certs/password.txt
  tlsSecurityProfile:
    type: Custom
    ciphers:
        - TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
        - TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
    minTLSVersion: VersionTLS13
dev_config:
  # config options specific to dev environment - launching OLS in local
  enable_dev_ui: true
  disable_auth: true
  disable_tls: true
  pyroscope_url: "https://pyroscope.pyroscope.svc.cluster.local:4040"
  enable_system_prompt_override: true
  # uvicorn_port_number: 8081
  # llm_params:
  #   temperature_override: 0
  # k8s_auth_token: optional_token_when_no_available_kube_config
