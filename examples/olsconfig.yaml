# This is a sample config for reference.
# Update properties with actual value; Ex: use the actual model name.
llm_providers:
  - name: my_openai
    type: openai
    url: "https://api.openai.com/v1"
    credentials_path: openai_api_key.txt
    models:
      - name: model-name-1
      - name: model-name-2
  - name: my_azure_openai
    type: azure_openai
    url: "https://myendpoint.openai.azure.com/"
    credentials_path: azure_openai_api_key.txt
    api_version: "2024-02-15-preview"
    deployment_name: my_azure_openai_deployment_name
    models:
      - name: model-name
  - name: my_watsonx
    type: watsonx
    url: "https://us-south.ml.cloud.ibm.com"
    credentials_path: watsonx_api_key.txt
    project_id: XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX
    models:
      - name: model-name
  - name: my_rhoai
    type: rhoai_vllm
    url: "http://localhost:8000/v1"
    credentials_path: rhoai_api_key.txt
    models:
      - name: mistral-7b-instruct-v0.3
  - name: my_rhelai
    type: rhelai_vllm
    url: "http://localhost:8000/v1"
    credentials_path: rhelai_api_key.txt
    models:
      - name: model-name
  - name: instructlab
    type: openai
    url: "http://localhost:8000/v1"
    credentials_path: openai_api_key.txt
    models:
      - name: model-name
ols_config:
  # max_workers: 1
  reference_content:
    indexes:
      - product_docs_index_path: "./vector_db/ocp_product_docs/4.17"
        product_docs_index_id: ocp-product-docs-4_17
      - product_docs_index_path: "./vector_db/user_application_docs/version_1"
        product_docs_index_id: user-application-docs-version_1
    embeddings_model_path: "./embeddings_model"
  conversation_cache:
    type: memory
    memory:
      max_entries: 1000
  logging_config:
    app_log_level: info
    lib_log_level: warning
    uvicorn_log_level: info
    suppress_metrics_in_log: false
    suppress_auth_checks_warning_in_log: false
  default_provider: my_watsonx
  default_model: model-name
  expire_llm_is_ready_persistent_state: -1
  # query_filters:
  #   - name: foo_filter
  #     pattern: '\b(?:foo)\b'
  #     replace_with: "deployment"
  #   - name: bar_filter
  #     pattern: '\b(?:bar)\b'
  #     replace_with: "openshift"
  # query_validation_method chooses, how the first query will be validated
  # supported values:
  #     "keyword"  - keyword based query validation (see ols/utils/keywords.py)
  #     "llm"      - LLM based query validation
  #     "disabled" - question validation is disabled (all questions will be marked as valid)
  query_validation_method: llm
  authentication_config:
    module: "k8s"
    k8s_cluster_api: "https://api.example.com:6443"
    k8s_ca_cert_path: "/Users/home/ca.crt"
    skip_tls_verification: false
  user_data_collection:
    feedback_disabled: false
    feedback_storage: "/tmp/data/feedback"
    transcripts_disabled: false
    transcripts_storage: "/tmp/data/transcripts"
    config_disabled: false
    config_storage: "/tmp/data/config"
  tls_config:
    tls_certificate_path: /app-root/certs/certificate.crt
    tls_key_path: /app-root/certs/private.key
    tls_key_password_path: /app-root/certs/password.txt
  tlsSecurityProfile:
    type: Custom
    ciphers:
        - TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
        - TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
    minTLSVersion: VersionTLS13
dev_config:
  # config options specific to dev environment - launching OLS in local
  enable_dev_ui: true
  disable_auth: true
  disable_tls: true
  pyroscope_url: "https://pyroscope.pyroscope.svc.cluster.local:4040"
  enable_system_prompt_override: true
  # uvicorn_port_number: 8081
  # llm_params:
  #   temperature_override: 0
  # k8s_auth_token: optional_token_when_no_available_kube_config

# MCP servers - enables tool calling if present
mcp_servers:
  - name: openshift
    transport: stdio
    stdio:
      command: python
      args: 
        - ./mcp_local/openshift.py
