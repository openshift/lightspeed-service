# Env vars for use w/ envsubst:
# $PROVIDER - the llm provider to use (e.g. openai)
# $PROVIDER_PROJECT_ID - for use with watsonx, can leave empty otherwise
# $PROVIDER_URL - set the provider api url/override the default, primarily for use with azure openai
# $PROVIDER_DEPLOYMENT_NAME - for use with azure openai, can leave empty otherwise
# $MODEL - the llm model to use from the provider (e.g. gpt-3.5.-turbo)

---
kind: ConfigMap
apiVersion: v1
immutable: false
metadata:
  name: olsconfig
data:
  olsconfig.yaml: |
    llm_providers:
      - name: "$PROVIDER"
        url: $PROVIDER_URL
        project_id: "$PROVIDER_PROJECT_ID"
        deployment_name: "$PROVIDER_DEPLOYMENT_NAME"
        credentials_path: /app-root/config/llmcreds/llmkey
        models:
          - name: "$MODEL"
    ols_config:
      reference_content:
        product_docs_index_path: "./vector-db/ocp-product-docs"
        product_docs_index_id: ocp-product-docs-4_15
        embeddings_model_path: "./embeddings_model"
      conversation_cache:
        type: memory
        memory:
          max_entries: 1000
      logging_config:
        app_log_level: debug
        lib_log_level: debug
      default_provider: "$PROVIDER"
      default_model: "$MODEL"
      query_filters:
        - name: foo_filter
          pattern: '\b(?:foo)\b'
          replace_with: "deployment"
        - name: bar_filter
          pattern: '\b(?:bar)\b'
          replace_with: "openshift"
      tls_config:
        tls_certificate_path: /app-root/certs/tls.crt
        tls_key_path: /app-root/certs/tls.key
      user_data_collection:
        feedback_disabled: false
        feedback_storage: "/app-root/ols-user-data/feedback"
    dev_config:
      enable_dev_ui: true
      disable_auth: false
      disable_tls: false
